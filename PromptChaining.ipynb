{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**README**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## About the notebook\n",
        "\n",
        "This notebook demonstrates a critical concept of Prompt Engineering called **Prompt Chaining**. It shows how we can leverage response from one prompt to chain to the next using stateful conversational capablity of Responses API, so an AI model can solve a complex, multi-step problem without a single bulky prompt, overflowing context window or latency overhead.<br>\n",
        "\n",
        "*Pre-requisites:* To run this notebook,\n",
        "\n",
        "- Refer to [Google Colab](https://colab.research.google.com/) to get started instantly, for *free* !\n",
        "\n",
        "- Download and open this notebook in Google Colab.\n",
        "\n",
        "- Get your access key to the [OpenAI](https://platform.openai.com/account/api-keys) API.<br>\n",
        "\n",
        "- Set up this access key under *secrets* in your Google Colab runtime environment. </br>\n",
        "How to configure API access keys in Colab:\n",
        "\n",
        "  - Go to the \"üîë\" icon in the left sidebar (Secrets).\n",
        "  - Click \"Add new secret\".\n",
        "  - For the name, use 'openai_api_key'.\n",
        "  - For the value, paste your OpenAI API key.\n",
        "  - Make sure \"Notebook access\" is enabled for this secret.\n",
        "\n",
        "- You are all set! Have fun!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Topic: Prompt Chaining - Transform Your AI Assistant into a Strategic Problem-Solving Partner\n",
        "#### Overview\n",
        "Transform generic AI responses into comprehensive, multi-dimensional solutions that address technical design, business impact, and implementation strategy simultaneously.<br>\n",
        "\n",
        "#### The Problem\n",
        "Single prompts typically produce surface-level, generic solutions when complex problems require deep analysis across multiple domains and stakeholder perspectives.<br>\n",
        "\n",
        "#### Example Use Case\n",
        "Challenge: \"Our Features system lacks real-time enrollment tracking, making customer debugging impossible.\"<br>\n",
        "\n",
        "This isn't just a technical issue, it's a multi-dimensional challenge requiring:\n",
        "<br>\n",
        "\n",
        "- System architecture understanding\n",
        "- User experience design\n",
        "- Dashboard requirements\n",
        "- Resource planning\n",
        "- Risk assessment\n",
        "\n",
        "Solution: Prompt Chaining Technique\n",
        "Prompt chaining creates intelligent conversation flows where each AI response builds on previous context, delivering actionable solutions that address real-world complexities.\n",
        "\n",
        "#### Implementation Framework\n",
        "*Link 1:* Problem Decomposition & System Analysis\n",
        "- Identify core issues and dependencies\n",
        "- Map current system limitations\n",
        "- Define success criteria\n",
        "\n",
        "*Link 2:* Technical Architecture & Data Requirements\n",
        "- Design system components\n",
        "- Define data flow and storage needs\n",
        "- Identify integration points\n",
        "\n",
        "*Link 3:* Dashboard Design & Implementation Roadmap\n",
        "- Create user interface specifications\n",
        "- Plan development phases\n",
        "- Define deployment strategy\n",
        "\n",
        "*Link 4:* Resource Estimation & Risk Mitigation\n",
        "- Calculate effort and timeline\n",
        "- Identify potential blockers\n",
        "- Plan contingency strategies\n",
        "<br>\n",
        "What do you think the response would be?<br>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "APe2Cbdnq3R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hands-on excercise using OpenAI API and GPT5\n",
        "# This hands-on exercise provides a prompt chaining example"
      ],
      "metadata": {
        "id": "eB2G9F02E3bt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About this excercise:\n",
        "</br>\n",
        "\n",
        "|Category|Description|\n",
        "|:--|:--|\n",
        "|Task |AI tool as a strategic advisor on problem solving|\n",
        "|Difficuly Level|Intermediate|\n",
        "|Skills|Python|\n",
        "</br>"
      ],
      "metadata": {
        "id": "lq2yhLJTFCY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import display and Markdown from IPython for formatted rendering of generated response\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "khTmIN_rkUIs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ],
      "metadata": {
        "id": "UxZZYIXxkY9T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access setup - This function allows secure access to user-defined secrets stored in the Colab environment, such as API keys."
      ],
      "metadata": {
        "id": "CGWIxkyDFuXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "0bEjJDChcj5E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import - interact with the OpenAI API, allows us to make requests to models like GPT-5"
      ],
      "metadata": {
        "id": "oTN-vr2lF0jv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4Gl-e7ArcfgG"
      },
      "outputs": [],
      "source": [
        "#make use of the stateful nature of OpenAI Responses api to learn from responses and debug easily\n",
        "#use conversational benefits of passing the response ID from the earlier conversation to the next without additional latency or broadening the context window\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=userdata.get('openai_api_key'))"
      ],
      "metadata": {
        "id": "8Ta8lYX3cqdz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Link 1:** Prepare basic prompt input and generate response."
      ],
      "metadata": {
        "id": "RJEp4zWNGQeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_problem = 'Our Features system lacks real-time enrollment tracking, making customer debugging impossible'"
      ],
      "metadata": {
        "id": "KoZ8oHYYGZzM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-5\",\n",
        "    input=f\"Build a presentation of total 5 slides, maximum 3 bullet points on each slide to solve problem {input_problem}. think step-by-step\"\n",
        ")"
      ],
      "metadata": {
        "id": "qDqAT4nxdAvO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "about_this_print='Following is how the generic model responds to a basic prompt:'\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s</div><br>' % about_this_print)\n",
        "\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s</div>' % response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "hDv1bWEIdebO",
        "outputId": "d0c6adee-5c01-4f77-822f-97f00e4d5974"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">Following is how the generic model responds to a basic prompt:</div><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">Slide 1 ‚Äî Problem & Goal\n- Today: no real-time enrollment tracking ‚Üí blind to customer state, slow debugging\n- Impact: long MTTR, escalations, erosion of trust and revenue risk\n- Goal: real-time, per-customer enrollment timeline enabling fast, self-serve debugging\n\nSlide 2 ‚Äî Key Requirements\n- Freshness ‚â§1 minute; ordered per user; idempotent with replay/backfill\n- Searchable by customer, feature, device; correlate with requests/releases\n- Secure by design: minimize PII, RBAC, retention/redaction policies\n\nSlide 3 ‚Äî Proposed Architecture\n- Instrument enrollment lifecycle events with correlation IDs (server + client SDKs)\n- Stream (Kafka/Kinesis/PubSub) ‚Üí process (dedupe/order/PII scrub) ‚Üí store (ClickHouse/Druid/Elastic) + hot cache (Redis)\n- Debug UI/API: per-customer timeline, env diff, search; anomaly alerts\n\nSlide 4 ‚Äî Implementation Plan (Step-by-Step)\n- Phase 1: define event schema/IDs; update SDKs; create topics; ship server-side events; baseline dashboards\n- Phase 2: build processors (idempotency, ordering, PII); backfill history; indexed queries + freshness/completeness metrics\n- Phase 3: ship Debug UI/APIs; alerts/runbooks/RBAC; staged rollout by tenant; set SLOs and on-call\n\nSlide 5 ‚Äî Success Metrics & Risks\n- SLOs: p95 freshness ‚â§60s; 99.9% completeness; search p95 ‚â§2s\n- Outcomes: MTTR -50%, support tickets -30%, on-call time -25%\n- Risks/mitigations: scale/ordering (idempotent keys, partitioning), privacy (schema review/redaction), SDK adoption (flags, gradual rollout)</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Link 2:** Start chaining with revised instruction for leadership presentation style."
      ],
      "metadata": {
        "id": "9P7r8Z2OHDi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = client.responses.create(\n",
        "    model=\"gpt-5\",\n",
        "    previous_response_id=response.id,\n",
        "    input=[{\"role\": \"user\", \"content\": \"make it fluent for product and business leadership teams\"}]\n",
        ")"
      ],
      "metadata": {
        "id": "rxR1iBl0dh_K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "about_this_print='Following is how the generic model responds to a chained prompt using earlier response id:'\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s</div><br>' % about_this_print)\n",
        "\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s</div>' % response2.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "qZwWkALmeIhb",
        "outputId": "3d7ec602-541a-4a91-af98-a9c772a16511"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">Following is how the generic model responds to a chained prompt using earlier response id:</div><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">Slide 1 ‚Äî Why this matters\n- Today we can‚Äôt see a customer‚Äôs real-time enrollment state, so issues linger and escalate\n- Business impact: longer MTTR, more support load, stalled deals, and trust risk with enterprise customers\n- Goal: a live, per-customer enrollment timeline that enables fast, self-serve resolution\n\nSlide 2 ‚Äî What ‚Äúgood‚Äù looks like\n- Fresh, reliable view (updates in under 1 minute), searchable by customer, feature, device\n- Simple tools for Support, CS, Sales, and Eng: one place to answer ‚Äúwhat happened and when?‚Äù\n- Enterprise-ready: minimal PII, access controls, and clear retention policies\n\nSlide 3 ‚Äî Solution at a glance\n- Capture key moments in the enrollment journey (attempt, decision, success/fail) with a shared ID\n- Stream events into a real-time store that orders, deduplicates, and readies data for search\n- Deliver a Debug Console and API: customer timeline, compare environments, alerts on anomalies\n\nSlide 4 ‚Äî Step-by-step rollout\n- Phase 1 (Foundations): finalize event schema; instrument core paths; basic dashboards (2‚Äì3 weeks)\n- Phase 2 (Build): real-time pipeline and searchable store; privacy/quality guards; backfill recent history (4‚Äì6 weeks)\n- Phase 3 (Launch): Debug Console + RBAC; SLOs and runbooks; staged rollout to top accounts (3‚Äì4 weeks)\n\nSlide 5 ‚Äî Outcomes, ROI, and risks\n- Target outcomes: MTTR down 50%, support tickets down 30%, on-call time down 25%\n- Investment: small cross-functional squad (PM, BE, Data, FE) + modest infra; payback via support savings and churn prevention\n- Key risks and mitigations: data scale/ordering (idempotent keys, partitions), privacy (schema review/redaction), adoption (training, champions, gradual rollout)</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Link 3:** Continue chaining with revised instruction for engineering  presentation style."
      ],
      "metadata": {
        "id": "CewHYi7fHSmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response3 = client.responses.create(\n",
        "    model=\"gpt-5\",\n",
        "    previous_response_id=response2.id,\n",
        "    input=[{\"role\": \"user\", \"content\": \"make it technical for engineering team to estimate the effort\"}]\n",
        ")"
      ],
      "metadata": {
        "id": "2taZd6TGeLaU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "about_this_print='Following is how the generic model responds to a chained prompt using earlier response id and revised instruction:'\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s</div><br>' % about_this_print)\n",
        "\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s</div>' % response3.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "BrJyMK-5et67",
        "outputId": "0a5a983c-1dae-46bd-9314-da0e7c0163b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">Following is how the generic model responds to a chained prompt using earlier response id and revised instruction:</div><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">Slide 1 ‚Äî Scope, schema, scale\n- Event types/fields: enrollment_attempt/decision/success/failure/state_change; keys: tenant_id, user_id_hash, device_id, feature_id, env, build/version, request_id, correlation_id, event_ts/ingest_ts, source, reason/error; no raw PII\n- Coverage: server evaluator + gateways; SDKs (Web/iOS/Android/Node) with retry/offline queue; clock skew tolerance ¬±10m\n- Scale assumptions: peak 5k RPS (~400M/day), ~400B/event (~160GB/day); hot retention 30d, cold archive 180d\n\nSlide 2 ‚Äî Ingestion and processing\n- Transport: Kafka (12‚Äì24 partitions, RF=3, acks=all, idempotent producers, linger/batch tuned) or Kinesis (‚â•10 shards); DLQ for parse/schema errors\n- Processor: dedupe by idempotency_key (tenant_id+request_id), order per (tenant_id,user/device_id) with 10m window; late/dup handling, PII scrubbing/redaction\n- Storage: ClickHouse (MergeTree; partition by toDate(event_ts); order by (tenant_id, user/device_id, event_ts)) with secondary indexes; Redis cache for hot timelines (TTL ~15m)\n\nSlide 3 ‚Äî APIs and contracts\n- Endpoints: GET /timelines (tenant_id, user|device, feature, from/to), GET /diff (envA vs envB), GET /search, POST /annotations; p95 ‚â§2s; 10k events cap/query\n- Semantics: eventual consistency with freshness SLO ‚â§60s; pagination, time-bucketed queries; materialized aggregates (last_state, event_counts)\n- Security: mTLS/JWT, RBAC roles (Support/Eng/Admin), per-tenant rate limits, audit logs; request/response size limits\n\nSlide 4 ‚Äî UI, observability, data quality\n- Debug Console: ordered timeline with filters (tenant/feature/device/env), event detail panes, correlation IDs linking to logs/traces/deploys; env-compare view\n- Observability: dashboards for lag, consumer liveness, DLQ rate, dedupe hit rate; SLOs freshness ‚â§60s, completeness ‚â•99.9%; alerting and runbooks\n- Data quality: schema registry with compatibility checks, canary validators, replay/backfill tool (topic‚Üístore), synthetic tenants and fixtures for tests\n\nSlide 5 ‚Äî Plan, estimates, risks\n- Team and schedule: 1 BE, 1 Data, 1 FE, 0.5 SRE; Phase 1 (2‚Äì3 wks) foundations; Phase 2 (4‚Äì6 wks) pipeline/APIs/SDKs; Phase 3 (3‚Äì4 wks) UI/RBAC/rollout\n- Dependencies/deliverables: Kafka/Kinesis + schema registry, ClickHouse cluster + Redis, SDK releases, load/capacity tests, alerts/runbooks, staged rollout to top tenants\n- Risks/mitigations: hot partitions (hash user/device + more partitions), privacy (PII redaction/review), SDK adoption (feature flags, gradual rollout), cost (tiered retention, compaction)</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Link 4:** Continue chaining with revised instruction for reasoning and impacting talking points."
      ],
      "metadata": {
        "id": "2F__drgYHl3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response4 = client.responses.create(\n",
        "    model='o4-mini',\n",
        "    input=[{\"role\": \"user\", \"content\": \"explain the effort\"}],\n",
        "    previous_response_id=response3.id,\n",
        "    reasoning={\"effort\": \"high\"},\n",
        "    max_output_tokens=10000,\n",
        "    stream=False\n",
        ")"
      ],
      "metadata": {
        "id": "Jvhe0TEIe5xj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "about_this_print='Following is how the reasoning optimized model explains the thought porcess:'\n",
        "printmd('<div style=\"background-color: #a8ee90; padding: 10px;\">%s</div><br>' % about_this_print)\n",
        "\n",
        "printmd('<div style=\"background-color: #a8ee90; padding: 10px;\">%s</div>' % response4.output_text)"
      ],
      "metadata": {
        "id": "FZwyySfBgglq",
        "outputId": "4c54c9b7-1351-4988-fe60-0910775e2184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: #a8ee90; padding: 10px;\">Following is how the reasoning optimized model explains the thought porcess:</div><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: #a8ee90; padding: 10px;\">Here‚Äôs a high-level breakdown of the engineering effort to deliver the real-time enrollment tracking system. All estimates assume a cross-functional team:  \n ‚Ä¢ Backend (BE) engineers  \n ‚Ä¢ Data engineers  \n ‚Ä¢ Frontend (FE) engineer  \n ‚Ä¢ SRE/infra engineer  \n\n1. Phase 1 ‚Äì Foundations (2‚Äì3 weeks) ~250 h  \n ‚Ä¢ Define event schema, idempotency keys, PII policy  \n    ‚Äì BE: 40 h; Data: 10 h  \n ‚Ä¢ Instrument server paths & SDKs (web, iOS, Android, Node)  \n    ‚Äì BE: 30 h; FE: 10 h  \n ‚Ä¢ Provision Kafka/Kinesis topics, schema registry  \n    ‚Äì SRE: 40 h  \n ‚Ä¢ Build basic producer pipelines & sanity dashboards  \n    ‚Äì BE: 40 h; Data: 20 h  \n ‚Ä¢ QA & integration tests  \n    ‚Äì BE/FE: 20 h  \n\n2. Phase 2 ‚Äì Real-time Pipeline & APIs (4‚Äì6 weeks) ~600 h  \n ‚Ä¢ Stream processors: dedupe, ordering window, PII scrub  \n    ‚Äì BE: 120 h; Data: 80 h  \n ‚Ä¢ Storage: ClickHouse schema, MergeTree tuning, Redis cache  \n    ‚Äì SRE: 40 h; Data: 40 h  \n ‚Ä¢ Build query APIs: /timelines, /search, /diff, POST annotations  \n    ‚Äì BE: 100 h  \n ‚Ä¢ SDK enhancements (retry/offline queue, correlation IDs)  \n    ‚Äì BE: 40 h; FE: 40 h  \n ‚Ä¢ End-to-end tests, backfill tooling, canary validations  \n    ‚Äì Data: 40 h; BE: 20 h; SRE: 20 h  \n\n3. Phase 3 ‚Äì Debug UI, Security, Rollout (3‚Äì4 weeks) ~350 h  \n ‚Ä¢ Debug Console UI: timeline view, filters, env-compare  \n    ‚Äì FE: 120 h; BE: 40 h for endpoints  \n ‚Ä¢ RBAC, mTLS/JWT, rate limits, audit-logging  \n    ‚Äì BE: 40 h; SRE: 20 h  \n ‚Ä¢ Observability: consumer lag, DLQ, SLO dashboards, alerts/runbooks  \n    ‚Äì SRE: 40 h  \n ‚Ä¢ Documentation, training, staged tenant rollout  \n    ‚Äì BE/FE/Data/SRE: 50 h  \n ‚Ä¢ Buffer for bug-fixes & iteration  \n    ‚Äì All: 40 h  \n\nTotal estimated effort ~1,200 engineering hours (~150 days).  \n‚Ä¢ If staffed as 2 BE, 0.5 Data, 0.5 FE, 0.5 SRE FTE ‚Üí ~3 calendar months  \n‚Ä¢ Key dependencies: infra capacity (Kafka, ClickHouse, Redis), SDK release cadence, schema-registry setup, tenant coordination for rollout.  \n\nThis breakdown will help the team size stories, plan sprints, and track progress against milestones.</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "##Congratulations on running this fun exercise!\n",
        "\n",
        "\n",
        "**Key Advantages**\n",
        "- Context Preservation & Knowledge Building\n",
        " - Each prompt builds on previous responses\n",
        " - Creates a cumulative knowledge base\n",
        " - Maintains understanding of specific constraints and objectives\n",
        "\n",
        "- Multi-Dimensional Problem Solving\n",
        " - Addresses different stakeholder needs simultaneously\n",
        " - Technical depth for engineers\n",
        " - Business impact for executives\n",
        " - Resource planning for project managers\n",
        " - Risk assessment for leadership\n",
        "\n",
        "**Expected Outcomes**\n",
        "\n",
        "- Faster Decision Making - Comprehensive analysis reduces back-and-forth\n",
        "- Better Stakeholder Alignment - Addresses all perspectives in a cohesive framework\n",
        "- Higher Implementation Success - Solutions account for real-world complexities\n",
        "- Strategic AI Partnership - AI evolves from tool to strategic advisor<br>\n",
        "\n",
        "You just proved it yourself!<br>\n",
        "\n",
        "\n",
        "**Fun Fact:** Did you notice that we could choose different models to chain the prompts?<br>\n",
        "\n",
        "\n",
        "\n",
        "**Extra Credit:**\n",
        "- Identify a Complex Problem - Multi-faceted challenges work best\n",
        "- Define Stakeholder Needs - What does each audience require?\n",
        "- Design Chain Structure - Plan 3-5 logical progression links\n",
        "- Execute Sequential Prompts - Build context with each interaction\n",
        "- Synthesize Results - Combine outputs into a comprehensive solution\n",
        "\n",
        "\n",
        "**Pro Tip:** Best Practices\n",
        "\n",
        "- Keep each link focused on a specific domain\n",
        "- Explicitly reference previous context in subsequent prompts\n",
        "- Design for different stakeholder audiences\n",
        "- Plan 3-5 links maximum for optimal results\n",
        "- Test chain logic before execution<br>\n",
        "\n",
        "\n",
        "If you are an AI Enthusiast, don't stop here - you could start with your first, basic RAG based on prompt chaining and a few more techniques we will cover soon. Happy Prompt Chaining!<br>"
      ],
      "metadata": {
        "id": "8I0BdH_tH-sQ"
      }
    }
  ]
}