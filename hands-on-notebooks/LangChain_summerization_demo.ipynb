{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**README**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## About the notebook\n",
        "\n",
        "\n",
        "\n",
        "This notebook demonstrates the power of LangChain to summarize documents using the \"Stuff\" and the \"Map Reduce\" techniques. <br>\n",
        "\n",
        "\n",
        "\n",
        "*Learning Objectives:* To understand,\n",
        "\n",
        "- Use Stuff vs Map-Reduce document processing\n",
        "\n",
        "- Implement both approaches with real code examples\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "*Prerequisites:*\n",
        "\n",
        "- Skills: Python 3+, Basics of LangChain concepts, Basics of prompt engineering\n",
        "- Difficuly Level: Beginner\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "*Quick Start:*<br>\n",
        "\n",
        "1. Clone the Repository\n",
        "\n",
        "```\n",
        "\n",
        "git clone https://github.com/PradnyaSA/MyAIExperiments.git\n",
        "\n",
        "cd hands-on-notebooks\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "2. Set-Up Access  \n",
        "\n",
        "   - Refer to [Google Colab](https://colab.research.google.com/) to get started instantly, for *free* !\n",
        "\n",
        "\n",
        "\n",
        "   - Get your access key to the [OpenAI](https://platform.openai.com/account/api-keys) API.\n",
        "\n",
        "   <br>\n",
        "\n",
        "\n",
        "\n",
        "3. To set up API keys in Colab: Go to the \"ðŸ”‘\" icon on the left sidebar (Secrets).\n",
        "\n",
        "    - Click \"Add new secret\".\n",
        "\n",
        "    - For the name, use 'openai_api_key'.\n",
        "\n",
        "    - For the value, paste your OpenAI API key.\n",
        "\n",
        "    - Make sure \"Notebook access\" is enabled for this secret.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "4. Run the notebook\n",
        "\n",
        "   - Open the notebook in Google Colab.\n",
        "\n",
        "   - Run each cell or run all.\n",
        "\n",
        "<br>\n",
        "\n",
        "You are all set!\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Topic: LangChain - Transform Your AI Assistant into a Document Summarizer tool\n",
        "\n",
        "#### Exercise 1: Basic Comparison (~15 minutes)\n",
        "\n",
        "Learn the fundamental differences between the the stuff and the map-reduce approaches:\n",
        "\n",
        "\n",
        "\n",
        "- Run both methods on the same dataset\n",
        "\n",
        "- Compare outputs and token usage *Extra Credit*\n",
        "\n",
        "- Understand the processing flow *Extra Credit*<br>\n",
        "\n",
        "\n",
        "\n",
        "Expected Output:\n",
        "\n",
        "\n",
        "\n",
        "- Side-by-side comparison of results\n",
        "\n",
        "- Token usage statistics *Extra Credit*\n",
        "\n",
        "- Performance metrics *Extra Credit*\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "#### Exercise 2: Scaling Challenge (~30 minutes) Extra Credit\n",
        "\n",
        "Learn the fundamental differences between the stuff and the map-reduce approaches:\n",
        "\n",
        "\n",
        "\n",
        "Discover when stuff approach fails but map-reduce succeeds:\n",
        "\n",
        "\n",
        "\n",
        "- Process an increasing number of documents\n",
        "\n",
        "- Hit context window limits\n",
        "\n",
        "- Measure performance degradation\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "Tasks:\n",
        "\n",
        "\n",
        "\n",
        "- Start with a few, say 5 documents - both methods work\n",
        "\n",
        "- Scale to +15 documents - stuff approach struggles\n",
        "\n",
        "- Scale to +35 documents - only map-reduce works<br>\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "#### Exercise 3: Custom Implementation (~45 minutes) Extra Credit\n",
        "\n",
        "Build your own document processing strategy:\n",
        "\n",
        "\n",
        "\n",
        "- Create custom prompts for your use case\n",
        "\n",
        "- Implement hybrid approaches\n",
        "\n",
        "- Optimize for your specific requirements<br>\n",
        "\n",
        "\n",
        "\n",
        "Challenges:\n",
        "\n",
        "\n",
        "\n",
        "- Design prompts for financial analysis\n",
        "\n",
        "- Handle different document types\n",
        "\n",
        "- Implement error handling and retries<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Performance Benchmarks - Extra Credit\n",
        "\n",
        "## Sample Results - Extra Credit\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "Stuff Documents Chain\n",
        "\n",
        "\n",
        "\n",
        "- How it works: Concatenates all documents into a single prompt\n",
        "\n",
        "- Pros: Fast, simple, maintains full context\n",
        "\n",
        "- Cons: Limited by context window, fails with many documents\n",
        "\n",
        "- Best for: Small document sets, cross-document analysis\n",
        "\n",
        "\n",
        "\n",
        "Map-Reduce Documents Chain\n",
        "\n",
        "\n",
        "\n",
        "- How it works: Process documents individually, then combine results\n",
        "\n",
        "- Pros: Scales to any number of documents, handles large datasets\n",
        "\n",
        "- Cons: More API calls, higher cost, may miss cross-document patterns\n",
        "\n",
        "- Best for: Large document sets, parallel processing needs\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "APe2Cbdnq3R7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lq2yhLJTFCY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import display and Markdown from IPython for formatted rendering of generated response\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "khTmIN_rkUIs"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ],
      "metadata": {
        "id": "UxZZYIXxkY9T"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access setup - This function allows secure access to user-defined secrets stored in the Colab environment, such as API keys."
      ],
      "metadata": {
        "id": "CGWIxkyDFuXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "0bEjJDChcj5E"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community pypdf langchain-openai tiktoken"
      ],
      "metadata": {
        "id": "E61x58MpMvSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629c8df9-c974-4347-b9cc-0f0bf9c800db"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.0.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.33)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.76)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.24)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.106.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "MYGksftyN7JA"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "example of a Summary Plan Description(SPD) for a popular fintech organization for demo purposes download and list"
      ],
      "metadata": {
        "id": "58IIAnjsLgzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.paypalbenefits.com/document/57\n",
        "!ls -lart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUMlGFOmORDN",
        "outputId": "729a2db3-1892-448f-eaef-2b433a8cb1d5"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-13 22:07:23--  https://www.paypalbenefits.com/document/57\n",
            "Resolving www.paypalbenefits.com (www.paypalbenefits.com)... 151.101.130.216, 151.101.194.216, 151.101.66.216, ...\n",
            "Connecting to www.paypalbenefits.com (www.paypalbenefits.com)|151.101.130.216|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 511430 (499K) [application/pdf]\n",
            "Saving to: â€˜57.9â€™\n",
            "\n",
            "57.9                100%[===================>] 499.44K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-09-13 22:07:24 (5.26 MB/s) - â€˜57.9â€™ saved [511430/511430]\n",
            "\n",
            "total 5016\n",
            "-rw-r--r-- 1 root root 511430 Mar 28  2024 57.9\n",
            "-rw-r--r-- 1 root root 511430 Mar 28  2024 57.8\n",
            "-rw-r--r-- 1 root root 511430 Mar 28  2024 57.7\n",
            "-rw-r--r-- 1 root root 511430 Mar 28  2024 57.6\n",
            "-rw-r--r-- 1 root root 511430 Mar 28  2024 57.5\n",
            "-rw-r--r-- 1 root root 511430 Mar 28  2024 57.4\n",
            "-rw-r--r-- 1 root root 511430 Mar 28  2024 57.3\n",
            "-rw-r--r-- 1 root root 511430 Mar 28  2024 57.2\n",
            "-rw-r--r-- 1 root root 511430 Mar 28  2024 57.1\n",
            "-rw-r--r-- 1 root root 511430 Mar 28  2024 57\n",
            "drwxr-xr-x 4 root root   4096 Sep  9 13:46 .config\n",
            "drwxr-xr-x 1 root root   4096 Sep  9 13:46 sample_data\n",
            "drwxr-xr-x 1 root root   4096 Sep 13 14:57 ..\n",
            "drwxr-xr-x 1 root root   4096 Sep 13 22:07 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-5\", api_key=userdata.get('openai_api_key'))"
      ],
      "metadata": {
        "id": "hDKSKS23SeP1"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import library specifically designed to load documents from pdf files"
      ],
      "metadata": {
        "id": "NuCPJJQ7LlQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "-UDOFzAiPGfd"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"57\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "7xfXJsW-48DU"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1: Basic Comparison (~15 minutes)"
      ],
      "metadata": {
        "id": "csGdNFmWPSgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "FORCkTm0L3dd"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain"
      ],
      "metadata": {
        "id": "BmwNsr6YQHI3"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How much does the company match for a 401(k), and what's the vesting schedule?\""
      ],
      "metadata": {
        "id": "6mmlDBmszlnz"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define stuff single prompt template\n",
        "prompt_template = \"\"\"You are an expert Analyzer of 401K documents. Based on following documents,\n",
        "write an answer this question: {question}\n",
        "Documents: {context}\n",
        "SUMMARY:\"\"\"\n"
      ],
      "metadata": {
        "id": "OfNgkxm1MXeW"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stuff_prompt = PromptTemplate(template=prompt_template,input_variables=[\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "71P_-Jgrc8Vz"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's give model some instructions and utilize LangChain's modules to summerize content of this pdf"
      ],
      "metadata": {
        "id": "LbNcgJ7-64ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the stuff chain using LCEL (LangChain Expression Language)\n",
        "stuff_chain = create_stuff_documents_chain(llm=llm,prompt=stuff_prompt)"
      ],
      "metadata": {
        "id": "-SXvW-JIOE40"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3v1Sn5R0VAyM"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = stuff_chain.invoke({\n",
        "    \"context\": pages[4:10], # only limiting pages for scope of this excercise is limited. In real-world, you can remove this limitation and scan all pages.\n",
        "     \"question\":question\n",
        "    })"
      ],
      "metadata": {
        "id": "S_X7gF0oQJlH"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "about_this_print=f'Following is how the model summerized the documents to answer the question: %s'\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s</div><br>' % about_this_print % question)\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s</div>' % res)"
      ],
      "metadata": {
        "id": "FZwyySfBgglq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "3b81b1b6-8b3b-4bce-9c5a-3a24b7be24c2"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">Following is how the model summerized the documents to answer the question: How much does the company match for a 401(k), and what's the vesting schedule?</div><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">- Company match: Dollar-for-dollar (100%) on your 401(k) contributions up to 4% of eligible compensation each allocation period. Catch-up contributions count toward the match.\n- Vesting: Safe Harbor matching contributions are 100% vested immediately. Your own 401(k) contributions are also 100% vested at all times.</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain"
      ],
      "metadata": {
        "id": "PlvrBEcJPsWw"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define map prompt template\n",
        "map_prompt_template = \"\"\"You are an expert Analyzer of 401K documents. Analyze following document section and extract key information relevant to the question: {question}\n",
        "Document section: {docs}\n",
        "ANALYSIS:\"\"\"\n"
      ],
      "metadata": {
        "id": "clCc6YicdyoI"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map prompt - processes each document individually\n",
        "map_prompt = PromptTemplate.from_template(\n",
        "    template=map_prompt_template\n",
        ")"
      ],
      "metadata": {
        "id": "utABW-AqS1tT"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define reduce prompt template\n",
        "reduce_prompt_template = \"\"\"You are an expert Analyzer of 401K documents. Based on the following analysis from multiple document sections,\n",
        "write a concise summary to answer this question: {question}\n",
        "Document analyses: {docs}\n",
        "ANALYSIS:\"\"\""
      ],
      "metadata": {
        "id": "45isdSnne7m3"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce prompt - combines all individual results\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_prompt_template)"
      ],
      "metadata": {
        "id": "ZII6BpW-UO1T"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the chain - Map step\n",
        "map_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=map_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "g_uLohmMVf5-"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the chain - Reduce step\n",
        "reduce_chain = LLMChain(\n",
        "       llm=llm,\n",
        "       prompt=reduce_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "nivzRRKPV1xR"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combine_documents_chain = StuffDocumentsChain(\n",
        "        llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
        ")"
      ],
      "metadata": {
        "id": "su7AWzQLWC-N"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    token_max=1000,\n",
        ")"
      ],
      "metadata": {
        "id": "VoGGM1RtIi7U"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full chain\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    llm_chain=map_chain,\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    document_variable_name=\"docs\",\n",
        "    return_intermediate_steps=False,\n",
        ")"
      ],
      "metadata": {
        "id": "U9QyetVaWkx0"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = pages[4:10]"
      ],
      "metadata": {
        "id": "GggA-9g4NYsu"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute map-reduce chain\n",
        "map_reduce_result = map_reduce_chain.invoke({'input_documents':documents,'question':question})"
      ],
      "metadata": {
        "id": "utEPJHAOW4ip"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "about_this_print=f'Following is how the model summerized the documents to answer the question: %s'\n",
        "printmd('<div style=\"background-color: #a8ee90; padding: 10px;\">%s</div><br>' % about_this_print % question)\n",
        "printmd('<div style=\"background-color: #a8ee90; padding: 10px;\">%s</div>' % map_reduce_result[\"output_text\"])"
      ],
      "metadata": {
        "id": "7ZsLX0ncXP-E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "975a385e-b7c5-4c9a-ff7a-4750af029500"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: #a8ee90; padding: 10px;\">Following is how the model summerized the documents to answer the question: How much does the company match for a 401(k), and what's the vesting schedule?</div><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: #a8ee90; padding: 10px;\">- Company match: Safe Harbor match of 100% of your 401(k) contributions on the first 4% of eligible compensation each allocation period (catch-up contributions included).\n- Vesting: Your own contributions are 100% vested immediately. The excerpts provided do not state the vesting for the employer match; Safe Harbor matches are typically immediately 100% vested, but please confirm in the planâ€™s Vesting section.</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "##Congratulations on running this fun exercise!\n",
        "This lab is designed to give you practical, hands-on experience with LangChain document processing. Take your time with each exercise and don't hesitate to experiment beyond the provided examples.\n",
        "\n",
        "#### Additional Resources\n",
        "- [Lang Chain](https://www.langchain.com/langchain)\n",
        "\n",
        "Happy Learning!"
      ],
      "metadata": {
        "id": "8I0BdH_tH-sQ"
      }
    }
  ]
}