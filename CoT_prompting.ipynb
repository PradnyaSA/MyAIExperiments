{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**README**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## About the notebook\n",
        "\n",
        "This notebook demonstrates a Prompt Engineering technique called **Chain-Of-Thought(CoT)** prompting. It shows how CoT prompting enhances an AI model's ability to solve complex, multi-step problems.<br>\n",
        "\n",
        "\n",
        "\n",
        "*Pre-requisites:* To run this notebook,\n",
        "\n",
        "- Refer to [Google Colab](https://colab.research.google.com/) to get started instantly, for *free* !\n",
        "\n",
        "- Download and open this notebook in Google Colab.\n",
        "\n",
        "- Get your access key to the [OpenAI](https://platform.openai.com/account/api-keys) API.<br>\n",
        "\n",
        "- Set up this access key under *secrets* in your Google Colab runtime environment. </br>\n",
        "How to configure API access keys in Colab:\n",
        "\n",
        "  - Go to the \"ðŸ”‘\" icon in the left sidebar (Secrets).\n",
        "  - Click \"Add new secret\".\n",
        "  - For the name, use 'openai_api_key'.\n",
        "  - For the value, paste your OpenAI API key.\n",
        "  - Make sure \"Notebook access\" is enabled for this secret.\n",
        "\n",
        "- You are all set! Have fun!\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Topic: Chain-Of-Thought Prompting\n",
        "\n",
        "Evolving models with sophisticated reasoning abilities thrive on techniques such as Chain-of-Thought(CoT), few-shot prompting.<br>\n",
        "\n",
        "CoT prompting technique enables complex reasoning capabilities through intermediate reasoning steps.<br>\n",
        "\n",
        "\n",
        "\n",
        "Let's look at an example of a mathematical question where CoT prompting takes the center stage:<br>\n",
        "\n",
        "```\n",
        "\n",
        "\"A customer servicing agent needs 10 minutes of talk time for each complex customer inquiry. How many minutes will 120 customer service agents need in a year, if each agent handles 10 complex inquiries per day? If each customer servicing agent saves 1 minute for each complex customer inquiry, how many minutes will be saved in a year?\"\n",
        "\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "**Without CoT:** Adding extra information to a zero-shot prompt can force even the general-purpose model to think-out-loud before generating a response *OR* adding examples like few-shot prompting can make the general-purpose model split the task into multiple steps.<br>\n",
        "\n",
        "\n",
        "\n",
        "**With CoT:** Setting a high reasoning effort can make an evolved, better reasoning model potentially generate more detailed and accurate CoT responses without any further instructions.<br><br>\n",
        "\n",
        "\n",
        "\n",
        "What do you think the responses with CoT would be? Let's find out!\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fC-NxhKbRHOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hands-on exercise using OpenAI API.\n",
        "# This is to showcase how CoT enables an AI model to articulate its reasoning behind the generated response, improving transparency."
      ],
      "metadata": {
        "id": "aA98r6bSg5ui"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About this excercise:\n",
        "</br>\n",
        "\n",
        "|Category|Description|\n",
        "|:--|:--|\n",
        "|Task |Solve a mathematical calculation problem step-by-step|\n",
        "|Difficuly Level|Beginner|\n",
        "|Skills|Python|\n",
        "</br>"
      ],
      "metadata": {
        "id": "t5fX29goEYf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import display and Markdown from IPython for formatted rendering of generated response\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "tOBg13jg816S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ],
      "metadata": {
        "id": "g4Ox90irebfo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access setup - This function allows secure access to user-defined secrets stored in the Colab environment, such as API keys."
      ],
      "metadata": {
        "id": "FSAEuTOnSndK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "NL9XsB60uMzL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import - interact with the OpenAI API, allows us to make requests to models"
      ],
      "metadata": {
        "id": "FM78qcTOSOdK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VXpCYVk5uGDG"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=userdata.get('openai_api_key'))"
      ],
      "metadata": {
        "id": "v8tFQuE3uNLQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attempt 1:** Add some extra information to the zero-shot prompt to force the model to think-out-loud before generating a response (in CoT style)."
      ],
      "metadata": {
        "id": "VkRhL7yEwyQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = \"A customer servicing agent needs 10 minutes of talk time for each complex customer inquiry. How many minutes will 120 customer servicing agents need in a year, if each agent is handling 10 complex inquiries per day?\""
      ],
      "metadata": {
        "id": "Dndyy04srTkB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message += \"If each customer servicing agent saves 1 minute for each complex customer inquiry, how many minutes will be saved in a year?\" #an additional detail to a zero-shot prompt."
      ],
      "metadata": {
        "id": "rcZ3YxU86Vdd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_simple = client.responses.create(\n",
        "    model='gpt-4',\n",
        "    input=message,\n",
        "    max_output_tokens=2480\n",
        ")"
      ],
      "metadata": {
        "id": "tciPs-iYG0ja"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the response generated by AI model for a given prompt"
      ],
      "metadata": {
        "id": "ulb4jh5SxBMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "about_this_print='Following is how the generic model responds with an additional detail added to a zero-shot prompt:'\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s<br>' % about_this_print)\n",
        "printmd('%s</div>' % response_simple.output_text)"
      ],
      "metadata": {
        "id": "Q1ys6iR7HyLf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "57065530-f17e-4301-bb10-d389ee9e3090"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">Following is how the generic model responds with an additional detail added to a zero-shot prompt:<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Firstly, calculate how many minutes each agent would spend a day: 10 inquiries * 10 minutes/inquiry = 100 minutes/agent/day\nThen, find out how many minutes this would be in a year for one agent: 100 minutes/agent/day * 365 days/year = 36,500 minutes/agent/year\n\nFor 120 agents: 36,500 minutes/agent/year * 120 agents = 4,380,000 minutes for 120 agents in a year\n\nIf each agent saves 1 minute per inquiry, they will save: 1 minute/inquiry * 10 inquiries/agent/day = 10 minutes/agent/day\nTo find out how many minutes this would be in a year: 10 minutes/agent/day * 365 days/year = 3,650 minutes/agent/year\n\nFor 120 agents, they will save: 3,650 minutes/agent/year * 120 agents = 438,000 minutes for 120 agents in a year.</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attempt 2:** Add examples like few-shot prompting so the model learns from examples and splits the task into multiple steps (in CoT style)"
      ],
      "metadata": {
        "id": "ShyOPBVixpUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_manual_cot = client.responses.create(\n",
        "    model='gpt-4',\n",
        "    instructions=\"You are a helpful assistant. You must answer in a warm, polite, and friendly way like you are talking to a 20 years old.\",\n",
        "    input=[\n",
        "            {\"role\": \"system\", \"content\": \"\"\"\n",
        "            You are a helpful assistant.\n",
        "\n",
        "            Below are examples of questions and how to calculate the answer\n",
        "\n",
        "              Example 1: Arithmetic Problem\n",
        "              Prompt: \"If a toy costs $20 and there is a 10% discount, how much does the toy cost after the discount?\"\n",
        "              Chain of Thought Answer:\n",
        "                Calculate the amount of discount: 10% of $20 is $2.\n",
        "                Subtract the discount from the original price: $20 - $2 = $18.\n",
        "                The toy costs $18 after the discount.\n",
        "\n",
        "              Example 2: Logic Puzzle\n",
        "              Prompt: \"There are four apples and you take away three. How many apples do you have?\"\n",
        "              Chain of Thought Answer:\n",
        "                You start with four apples.\n",
        "                You take away three apples.\n",
        "                After taking three, you now have those three apples.\n",
        "                You have 3 apples\n",
        "\n",
        "              \"\"\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Answer the following question: {message}\"}\n",
        "        ],\n",
        "    max_output_tokens=2480\n",
        ")"
      ],
      "metadata": {
        "id": "bWkkGy5yZPet"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "about_this_print='Following is how the generic model responds with added few-shots to prompt:'\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s<br>' % about_this_print)\n",
        "printmd('%s</div>' % response_manual_cot.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "uytJB4R3Z20y",
        "outputId": "af04163d-745c-4f21-cd19-be99187f0095"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">Following is how the generic model responds with added few-shots to prompt:<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let's break this down:\n\nA customer service agent needs 10 minutes for each complex inquiry. Therefore, if each agent handles 10 complex inquiries each day, that will be 10 minutes * 10 inquiries = 100 minutes per day.\n\nIf we have 120 agents, they will therefore require 120 agents * 100 minutes/day/agent = 12,000 minutes per day in total.\n\nIn a year, with 365 days, the total minutes needed will be 12,000 minutes/day * 365 days/year = 4,380,000 minutes/year.\n\nIf each customer service agent saves 1 minute for each complex customer inquiry, the total minutes saved per agent per day would be 1 minute/inquiry * 10 inquiries/day/agent = 10 minutes/day/agent.\n\nFor 120 agents, they would collectively save 120 agents * 10 minutes/day/agent = 1,200 minutes/day.\n\nOver a year, the total minutes saved by all agents would then be 1,200 minutes/day * 365 days/year = 438,000 minutes/year.\n\nI hope this helps! If you need any more clarification, please don't hesitate to ask.</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attempt 3:** Set reasoning effort to high, so the model articulates the reasoning in CoT style and provides a more accurate CoT response."
      ],
      "metadata": {
        "id": "bT4-kkxZaBpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_auto_cot = client.responses.create(\n",
        "    model='o4-mini', #a smaller model optimized for fast, cost-efficient reasoning, capable of processing both text and images\n",
        "    input=[\n",
        "        {\"role\": \"system\", \"content\": \"Think step-by-step to solve the problem.\"},\n",
        "        {\"role\": \"user\", \"content\": message}\n",
        "    ],\n",
        "    reasoning={\"effort\": \"high\"},\n",
        "    max_output_tokens=5000,\n",
        "    stream=False\n",
        ")"
      ],
      "metadata": {
        "id": "t_PoApy7uPbY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the response generated by AI model for a given prompt"
      ],
      "metadata": {
        "id": "6lVSke2kyAk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "about_this_print='Following is how the evolved reasoning model responds to a CoT prompt:'\n",
        "printmd('<div style=\"background-color: #a8ee90; padding: 10px;\">%s<br>' % about_this_print)\n",
        "printmd('%s</div>' % response_auto_cot.output_text)"
      ],
      "metadata": {
        "id": "5CnMi0TFud6k",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "a02b8138-64c3-4ecd-8955-be0b771b1df2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: #a8ee90; padding: 10px;\">Following is how the evolved reasoning model responds to a CoT prompt:<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Letâ€™s assume each agent works every day of a (365-day) year.  \n\n1. Inquiries per day  \n   = 120 agents Ã— 10 inquiries/agent  \n   = 1 200 inquiries/day  \n\n2. Talk-time needed per day  \n   = 1 200 inquiries Ã— 10 min/inquiry  \n   = 12 000 minutes/day  \n\n3. Annual talk-time needed  \n   = 12 000 min/day Ã— 365 days  \n   = 4 380 000 minutes per year  \n\n4. Time saved per day (1 min saved per inquiry)  \n   = 1 200 inquiries Ã— 1 min/inquiry  \n   = 1 200 minutes/day  \n\n5. Annual time saved  \n   = 1 200 min/day Ã— 365 days  \n   = 438 000 minutes per year  \n\nSo over a 365-day year:\nâ€¢ Total talk time required = 4 380 000 minutes  \nâ€¢ Total minutes saved    =   438 000 minutes</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##Congratulations on running this fun exercise!\n",
        "\n",
        "\n",
        "CoT prompting enables the AI model to articulate reasoning, making it visible to the user and helping build confidence. The experience quality of working with AI improves, and this technique extends easily across domains for complex tasks, generating more accurate CoT responses. You just proved it yourself!<br>\n",
        "\n",
        "\n",
        "**Fun Fact:** Did you notice, we chose different models to show the difference in results?<br>\n",
        "\n",
        "**Extra Credit:** Try variations on effort and observe different responses. <br>\n",
        "\n",
        "**Pro Tip:** The true power of CoT is better realized when combined with other techniques like *few-shot* prompting. Don't forget the cost optimization benefit either!<br><br>\n",
        "\n",
        "If you are an AI Enthusiast, don't stop here - you could start with your first, simple chatbot application based on CoT. Try CoT prompting today!<br>"
      ],
      "metadata": {
        "id": "LTIH5nRkx4b5"
      }
    }
  ]
}