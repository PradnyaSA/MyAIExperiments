{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**README**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## About the notebook\n",
        "\n",
        "This notebook demonstrates a Prompt Engineering technique called **Chain-Of-Thought(CoT)** prompting. It shows how CoT prompting enhances an AI model's ability to solve complex, multi-step problems.<br>\n",
        "\n",
        "\n",
        "\n",
        "*Pre-requisites:* To run this notebook,\n",
        "\n",
        "- Refer to [Google Colab](https://colab.research.google.com/) to get started instantly, for *free* !\n",
        "\n",
        "- Download and open this notebook in Google Colab.\n",
        "\n",
        "- Get your access key to the [OpenAI](https://platform.openai.com/account/api-keys) API.<br>\n",
        "\n",
        "- Set up this access key under *secrets* in your Google Colab runtime environment. </br>\n",
        "How to configure API access keys in Colab:\n",
        "\n",
        "  - Go to the \"üîë\" icon in the left sidebar (Secrets).\n",
        "  - Click \"Add new secret\".\n",
        "  - For the name, use 'openai_api_key'.\n",
        "  - For the value, paste your OpenAI API key.\n",
        "  - Make sure \"Notebook access\" is enabled for this secret.\n",
        "\n",
        "- You are all set! Have fun!\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Topic: Chain-Of-Thought Prompting\n",
        "\n",
        "Evolving models with sophisticated reasoning abilities thrive on techniques such as Chain-of-Thought(CoT), few-shot prompting.<br>\n",
        "\n",
        "CoT prompting technique enables complex reasoning capabilities through intermediate reasoning steps.<br>\n",
        "\n",
        "\n",
        "\n",
        "Let's look at an example of a mathematical question where CoT prompting takes the center stage:<br>\n",
        "\n",
        "```\n",
        "\n",
        "\"A customer servicing agent needs 10 minutes of talk time for each complex customer inquiry. How many minutes will 120 customer service agents need in a year, if each agent handles 10 complex inquiries per day? If each customer servicing agent saves 1 minute for each complex customer inquiry, how many minutes will be saved in a year?\"\n",
        "\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "**Without CoT:** Adding extra information to a zero-shot prompt can force even the general-purpose model to think-out-loud before generating a response *OR* adding examples like few-shot prompting can make the general-purpose model split the task into multiple steps.<br>\n",
        "\n",
        "\n",
        "\n",
        "**With CoT:** Setting a high reasoning effort can make an evolved, better reasoning model potentially generate more detailed and accurate CoT responses without any further instructions.<br><br>\n",
        "\n",
        "\n",
        "\n",
        "What do you think the responses with CoT would be? Let's find out!\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fC-NxhKbRHOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hands-on exercise using OpenAI API.\n",
        "# This is to showcase how CoT enables an AI model to articulate its reasoning behind the generated response, improving transparency."
      ],
      "metadata": {
        "id": "aA98r6bSg5ui"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About this excercise:\n",
        "</br>\n",
        "\n",
        "|Category|Description|\n",
        "|:--|:--|\n",
        "|Task |Solve a mathematical calculation problem step-by-step|\n",
        "|Difficuly Level|Beginner|\n",
        "|Skills|Python|\n",
        "</br>"
      ],
      "metadata": {
        "id": "t5fX29goEYf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import display and Markdown from IPython for formatted rendering of generated response\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "tOBg13jg816S"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ],
      "metadata": {
        "id": "g4Ox90irebfo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access setup - This function allows secure access to user-defined secrets stored in the Colab environment, such as API keys."
      ],
      "metadata": {
        "id": "FSAEuTOnSndK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "NL9XsB60uMzL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import - interact with the OpenAI API, allows us to make requests to models"
      ],
      "metadata": {
        "id": "FM78qcTOSOdK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VXpCYVk5uGDG"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=userdata.get('openai_api_key'))"
      ],
      "metadata": {
        "id": "v8tFQuE3uNLQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attempt 1:** Add some extra information to the zero-shot prompt to force the model to think-out-loud before generating a response (in CoT style)."
      ],
      "metadata": {
        "id": "VkRhL7yEwyQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = \"A customer servicing agent needs 10 minutes of talk time for each complex customer inquiry. How many minutes will 120 customer servicing agents need in a year, if each agent is handling 10 complex inquiries per day?\""
      ],
      "metadata": {
        "id": "Dndyy04srTkB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message += \"If each customer servicing agent saves 1 minute for each complex customer inquiry, how many minutes will be saved in a year?\" #an additional detail to a zero-shot prompt."
      ],
      "metadata": {
        "id": "rcZ3YxU86Vdd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_simple = client.responses.create(\n",
        "    model='gpt-4',\n",
        "    input=message,\n",
        "    max_output_tokens=2480\n",
        ")"
      ],
      "metadata": {
        "id": "tciPs-iYG0ja"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the response generated by AI model for a given prompt"
      ],
      "metadata": {
        "id": "ulb4jh5SxBMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "about_this_print='Following is how the generic model responds with an additional detail added to a zero-shot prompt:'\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s</div><br>' % about_this_print)\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s</div>' % response_simple.output_text)"
      ],
      "metadata": {
        "id": "Q1ys6iR7HyLf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "882ff280-ffa2-4bac-fe6a-bcb134e1d8a0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">Following is how the generic model responds with an additional detail added to a zero-shot prompt:</div><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">First, calculate how many minutes will 120 customer servicing agents need in a year:\n\nThey each spend 10 minutes x 10 inquiries = <<10*10=100>>100 minutes per day.\nSo, the total time spent daily by 120 agents is 100 minutes/agent x 120 agents = <<100*120=12000>>12000 minutes.\nSince there are about 365 days in a year, the total time spent in a year is 12000 minutes/day x 365 days = 4,380,000 minutes.\n\nSecond, calculate how many minutes will be saved in a year if each customer servicing agent saves 1 minute for each complex customer inquiry:\n\nThis makes a total saving of 1 minute/inquiry x 10 inquiries = <<1*10=10>>10 minutes per day per agent.\nNow, let's multiply that by the number of agents and days in the year: \n10 minutes/agent/day x 120 agents x 365 days = 438,000 minutes.</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attempt 2:** Add examples like few-shot prompting so the model learns from examples and splits the task into multiple steps (in CoT style)"
      ],
      "metadata": {
        "id": "ShyOPBVixpUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_manual_cot = client.responses.create(\n",
        "    model='gpt-4',\n",
        "    instructions=\"You are a helpful assistant. You must answer in a warm, polite, and friendly way like you are talking to a 20 years old.\",\n",
        "    input=[\n",
        "            {\"role\": \"system\", \"content\": \"\"\"\n",
        "            You are a helpful assistant.\n",
        "\n",
        "            Below are examples of questions and how to calculate the answer\n",
        "\n",
        "              Example 1: Arithmetic Problem\n",
        "              Prompt: \"If a toy costs $20 and there is a 10% discount, how much does the toy cost after the discount?\"\n",
        "              Chain of Thought Answer:\n",
        "                Calculate the amount of discount: 10% of $20 is $2.\n",
        "                Subtract the discount from the original price: $20 - $2 = $18.\n",
        "                The toy costs $18 after the discount.\n",
        "\n",
        "              Example 2: Logic Puzzle\n",
        "              Prompt: \"There are four apples and you take away three. How many apples do you have?\"\n",
        "              Chain of Thought Answer:\n",
        "                You start with four apples.\n",
        "                You take away three apples.\n",
        "                After taking three, you now have those three apples.\n",
        "                You have 3 apples\n",
        "\n",
        "              \"\"\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Answer the following question: {message}\"}\n",
        "        ],\n",
        "    max_output_tokens=2480\n",
        ")"
      ],
      "metadata": {
        "id": "bWkkGy5yZPet"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "about_this_print='Following is how the generic model responds with added few-shots to prompt:'\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s</div><br>' % about_this_print)\n",
        "printmd('<div style=\"background-color: lightblue; padding: 10px;\">%s</div>' % response_manual_cot.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "uytJB4R3Z20y",
        "outputId": "fab966af-1927-48d5-e03f-b9ae74647835"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">Following is how the generic model responds with added few-shots to prompt:</div><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: lightblue; padding: 10px;\">That's a pretty interesting question!\n\nTo handle 10 inquiries each day, a single customer service agent will need 10 inquiries x 10 minutes = 100 minutes per day.\n\nWith 120 agents, the total time needed per day would be 120 agents x 100 minutes = 12,000 minutes.\n\nIn a year (excluding weekends and assuming they work for 5 days a week for 52 weeks), this results in 12,000 minutes/day x 5 days/week x 52 weeks/year = 3,120,000 minutes. That's an incredible amount of time spent answering customer inquiries!\n\nNow, if each agent saves 1 minute for each customer query, this results in a saving of 10 inquiries x 1 minute saved = 10 minutes saved per agent per day.\n\nFor 120 agents, this results in a daily saving of 120 agents x 10 minutes = 1,200 minutes.\n\nOver the course of a year (again excluding weekends and assuming they work 5 days a week for 52 weeks), the total time saved would be 1,200 minutes/day x 5 days/week x 52 weeks/year = 312,000 minutes.\n\nIsn't it amazing how saving just one minute for each customer inquiry can add up over time? Every minute truly counts!</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attempt 3:** Set reasoning effort to high, so the model articulates the reasoning in CoT style and provides a more accurate CoT response."
      ],
      "metadata": {
        "id": "bT4-kkxZaBpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_auto_cot = client.responses.create(\n",
        "    model='o4-mini', #a smaller model optimized for fast, cost-efficient reasoning, capable of processing both text and images\n",
        "    input=[\n",
        "        {\"role\": \"system\", \"content\": \"Think step-by-step to solve the problem.\"},\n",
        "        {\"role\": \"user\", \"content\": message}\n",
        "    ],\n",
        "    reasoning={\"effort\": \"high\"},\n",
        "    max_output_tokens=5000,\n",
        "    stream=False\n",
        ")"
      ],
      "metadata": {
        "id": "t_PoApy7uPbY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the response generated by AI model for a given prompt"
      ],
      "metadata": {
        "id": "6lVSke2kyAk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "about_this_print='Following is how the evolved reasoning model responds to a CoT prompt:'\n",
        "printmd('<div style=\"background-color: #a8ee90; padding: 10px;\">%s</div>' % about_this_print)\n",
        "printmd('<div style=\"background-color: #a8ee90; padding: 10px;\">%s</div>' % response_auto_cot.output_text)"
      ],
      "metadata": {
        "id": "5CnMi0TFud6k",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "outputId": "db11e29e-0ad3-42f3-be06-83981f1a3bc1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: #a8ee90; padding: 10px;\">Following is how the evolved reasoning model responds to a CoT prompt:</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<div style=\"background-color: #a8ee90; padding: 10px;\">Let N = number of agents = 120  \nq = inquiries per agent per day = 10  \nt = talk‚Äêtime per inquiry = 10 min  \nŒît = time saved per inquiry = 1 min  \nAssume they work D days per year.  \n\n1. Total minutes needed per day  \n   = N¬∑q¬∑t  \n   = 120¬∑10¬∑10  \n   = 12 000 min/day  \n\n2. Total minutes needed per year  \n   = 12 000¬∑D  \n   ‚Äì If D = 365 ‚áí 12 000¬∑365 = 4 380 000 min  \n   ‚Äì If D = 250 (approx. working days) ‚áí 12 000¬∑250 = 3 000 000 min  \n\n3. Minutes saved per day  \n   = N¬∑q¬∑Œît  \n   = 120¬∑10¬∑1  \n   = 1 200 min/day  \n\n4. Minutes saved per year  \n   = 1 200¬∑D  \n   ‚Äì If D = 365 ‚áí 1 200¬∑365 = 438 000 min  \n   ‚Äì If D = 250 ‚áí 1 200¬∑250 = 300 000 min  \n\nAnswer (using full‚Äêyear = 365 days):  \n‚Ä¢ 4 380 000 minutes of talk time required per year  \n‚Ä¢ 438 000 minutes saved per year  \n\nAnswer (using 250 working days):  \n‚Ä¢ 3 000 000 minutes required per year  \n‚Ä¢ 300 000 minutes saved per year</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##Congratulations on running this fun exercise!\n",
        "\n",
        "\n",
        "CoT prompting enables the AI model to articulate reasoning, making it visible to the user and helping build confidence. The experience quality of working with AI improves, and this technique extends easily across domains for complex tasks, generating more accurate CoT responses. You just proved it yourself!<br>\n",
        "\n",
        "\n",
        "**Fun Fact:** Did you notice, we chose different models to show the difference in results?<br>\n",
        "\n",
        "**Extra Credit:** Try variations on effort and observe different responses. <br>\n",
        "\n",
        "**Pro Tip:** The true power of CoT is better realized when combined with other techniques like *few-shot* prompting. Don't forget the cost optimization benefit either!<br><br>\n",
        "\n",
        "If you are an AI Enthusiast, don't stop here - you could start with your first, simple chatbot application based on CoT. Try CoT prompting today!<br>"
      ],
      "metadata": {
        "id": "LTIH5nRkx4b5"
      }
    }
  ]
}